{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Dropout, MaxPooling1D, Activation, Flatten\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes stop words.\n",
    "def text_tokens(tweets,text_parse):\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        \n",
    "        token_list = []\n",
    "        tokens = tknzr.tokenize(tweet)\n",
    "        \n",
    "        for token in tokens:\n",
    "            \n",
    "            token_value=token.lower()\n",
    "            \n",
    "            if token_value not in text_parse:\n",
    "                \n",
    "                token_list.append(token_value)\n",
    "        \n",
    "        tweet_tokens.append(token_list)\n",
    "    \n",
    "    return tweet_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates vocabulary.\n",
    "def vocab_build(token_list):\n",
    "    \n",
    "    vocab = defaultdict()\n",
    "    feature_vocab = defaultdict()\n",
    "    vocab.default_factory = vocab.__len__\n",
    "    \n",
    "    for tokens in token_list:\n",
    "        \n",
    "        for word in tokens:\n",
    "                \n",
    "            vocab[word]\n",
    "            feature_vocab[vocab[word]] = word\n",
    "    \n",
    "    return vocab,feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes document term frequency for vocabulary.\n",
    "def document_build(token_list,vocab):\n",
    "    \n",
    "    document_term = {}\n",
    "    \n",
    "    for term,_ in vocab.items():\n",
    "    \n",
    "        document_term[term] = 0\n",
    "    \n",
    "    for tokens in token_list:\n",
    "        \n",
    "        for word in tokens:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                document_term[word] += 1\n",
    "            \n",
    "            except:\n",
    "                \n",
    "                continue\n",
    "    \n",
    "    return document_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates TF-IDF for all samples.\n",
    "def total_importance(token_list,vocab):\n",
    "    \n",
    "    ti = {}\n",
    "    document_term = document_build(token_list,vocab)\n",
    "    documents = len(token_list)\n",
    "    \n",
    "    for idx,tokens in enumerate(token_list):\n",
    "        \n",
    "        ti[idx] = {}\n",
    "        frequency = nltk.FreqDist(tokens)\n",
    "        \n",
    "        for term,_ in frequency.items():\n",
    "            \n",
    "            try:\n",
    "            \n",
    "                feature_idx = vocab[term]\n",
    "                ti[idx][feature_idx] = frequency[term] * np.log(documents/document_term[term])\n",
    "            \n",
    "            except:\n",
    "                \n",
    "                continue\n",
    "    \n",
    "    return ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads word embeddings from GloVe trained model.\n",
    "def get_glove_embeddings(filename):\n",
    "    \n",
    "    fh = open(filename,'r',buffering=4096,encoding='UTF-8')\n",
    "    \n",
    "    word_embeddings = {}\n",
    "    vec_float = np.vectorize(float)\n",
    "    \n",
    "    for line in fh:\n",
    "        \n",
    "        vec = line.split()\n",
    "        \n",
    "        word_embeddings[vec[0]] = vec_float(vec[1:])\n",
    "    \n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads word embeddings from Word2Vec trained model.\n",
    "def get_w2v_embeddings(filename):\n",
    "    \n",
    "    with open(filename,'r') as f:\n",
    "        \n",
    "        data = json.load(f)\n",
    "    \n",
    "    words = list(data.keys())\n",
    "    w2v_embeddings = {}\n",
    "    array = np.vectorize(float)\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        w2v_embeddings[word] = array(data[word])\n",
    "    \n",
    "    return w2v_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads word embeddings from FastText trained model.\n",
    "def get_fast_text_embeddings(filename):\n",
    "    \n",
    "    fh = open(filename,'r',buffering=4096,encoding='UTF-8')\n",
    "    \n",
    "    fast_text_embeddings = {}\n",
    "    array = np.vectorize(float)\n",
    "    \n",
    "    for idx,line in enumerate(fh):\n",
    "        \n",
    "        vec = line.split()\n",
    "        fast_text_embeddings[vec[0]] = array(vec[1:])\n",
    "    \n",
    "    return fast_text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeds top 14 most important words per tweet based on TF-IDF (Avg token count is 14). \n",
    "def embed_tweets(token_list,w2v_embeddings,ti,vocab,word_number):\n",
    "    \n",
    "    X_embed = np.zeros((len(token_list),word_number,300))\n",
    "    \n",
    "    for idx,tokens in enumerate(token_list):\n",
    "        \n",
    "        words = []\n",
    "        words_ti = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            \n",
    "            words.append(token)\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                words_ti.append(ti[idx][vocab[token]])\n",
    "            \n",
    "            except:\n",
    "                \n",
    "                words_ti.append(0)\n",
    "        \n",
    "        if len(words) < word_number:\n",
    "            \n",
    "            words += ['Pad']*(word_number-len(words))\n",
    "            words_ti += [0]*(word_number-len(words_ti))\n",
    "            idx_sort = np.flip(np.argsort(words_ti),axis=0)\n",
    "            words = list(np.array(words)[idx_sort])\n",
    "            words_ti = list(np.array(words_ti)[idx_sort])\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            idx_sort = np.flip(np.argsort(words_ti)[-word_number:],axis=0)\n",
    "            words = list(np.array(words)[idx_sort])\n",
    "            words_ti = list(np.array(words_ti)[idx_sort])\n",
    "        \n",
    "        embeddings = []\n",
    "        \n",
    "        for word_idx,word in enumerate(words):\n",
    "            \n",
    "            embeddings.append(words_ti[word_idx]*w2v_embeddings.get(word,np.zeros(300)))\n",
    "        \n",
    "        embeddings = np.array(embeddings)\n",
    "        X_embed[idx] = embeddings\n",
    "   \n",
    "    return X_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates features.\n",
    "def feature_generation(df,stop_words):\n",
    "    \n",
    "    df['tokens'] = text_tokens(df.text.values,stop_words)\n",
    "    \n",
    "    df = pd.DataFrame(df.values,index=list(range(len(df))),columns=df.columns)\n",
    "    token_idx = [idx for idx,token in enumerate(df.tokens.values) if len(token) == 0]\n",
    "    df = df.drop(token_idx,axis=0)\n",
    "    \n",
    "    df['daily_diff'] = df.close-df.open\n",
    "    df['up_down'] = [1 if diff > 0 else 0 for diff in df.daily_diff]\n",
    "\n",
    "    data = df.values\n",
    "    \n",
    "    xtr,xte,ytr,yte = train_test_split(data[:,:-1],data[:,-1],test_size=.2,stratify=data[:,-1])\n",
    "    xtr,x_val,ytr,y_val = train_test_split(xtr,ytr,test_size=.2,stratify=ytr)\n",
    "    \n",
    "    df_train = pd.DataFrame(xtr,columns=list(df.columns[:-1]))\n",
    "    df_test = pd.DataFrame(xte,columns=list(df.columns[:-1]))\n",
    "    df_val = pd.DataFrame(x_val,columns=list(df.columns[:-1]))\n",
    "    \n",
    "    train_tokens = df_train.tokens.values\n",
    "    test_tokens = df_test.tokens.values\n",
    "    val_tokens = df_val.tokens.values\n",
    "    \n",
    "    vocab,feature_vocab = vocab_build(train_tokens)\n",
    "        \n",
    "    w2v_embeddings = get_w2v_embeddings('../w2v_embeddings.json')\n",
    "    \n",
    "    ti_train = total_importance(train_tokens,vocab)\n",
    "    ti_test = total_importance(test_tokens,vocab)\n",
    "    ti_val = total_importance(val_tokens,vocab)\n",
    "    \n",
    "    train_embed = embed_tweets(train_tokens,w2v_embeddings,ti_train,vocab,14)\n",
    "    test_embed = embed_tweets(test_tokens,w2v_embeddings,ti_test,vocab,14)\n",
    "    val_embed = embed_tweets(val_tokens,w2v_embeddings,ti_val,vocab,14)\n",
    "    \n",
    "    ytr = np.array([ytr.astype(int)]).T\n",
    "    yte = np.array([yte.astype(int)]).T\n",
    "    y_val = np.array([y_val.astype(int)]).T\n",
    "    \n",
    "    encoder = OneHotEncoder()\n",
    "    \n",
    "    y_train = encoder.fit_transform(ytr)\n",
    "    y_test = encoder.fit_transform(yte)\n",
    "    y_val = encoder.fit_transform(y_val)\n",
    "    \n",
    "    return train_embed,test_embed,val_embed,y_train,y_test,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = RegexpTokenizer('[a-zA-Z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_parse = [mark for mark in punctuation]\n",
    "word_parse = stopwords.words('english')\n",
    "stop_words = punctuation_parse + word_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../twitter/twitter_stock_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,x_val,y_train,y_test,y_val = feature_generation(df,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each embedding feature is treated as a seperate channel.\n",
    "\n",
    "# Each word per filter is a summation of the linear combination of three hundred seperate weights and\n",
    "# all embedding features.\n",
    "\n",
    "# Combination is then normalized when passed through ReLu activation.\n",
    "\n",
    "# At maxpooling stage two neighboring words are compared per feature and the feature of the word contributing the \n",
    "# largest value to the feature map is retained and the process repeated until the combination of features \n",
    "# from all words contributing the most per tweet remains and its weights used as a tweet generalization. \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(300,kernel_size=1,strides=1,input_shape=(14,300)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Conv1D(300,kernel_size=1,strides=1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Conv1D(300,kernel_size=1,strides=1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 14, 300)           90300     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 300)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 7, 300)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 300)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 7, 300)            90300     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 300)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 3, 300)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 300)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 3, 300)            90300     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3, 300)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 271,502\n",
      "Trainable params: 271,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_embedding_model.json','w') as f:\n",
    "    json.dump(model_json,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping('val_acc',patience=3,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint= ModelCheckpoint('word_embedding_weights.hdf5','val_acc',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 157654 samples, validate on 39414 samples\n",
      "Epoch 1/100\n",
      "157654/157654 [==============================] - 237s 2ms/step - loss: 3.9387 - acc: 0.6135 - val_loss: 0.4731 - val_acc: 0.7535\n",
      "Epoch 2/100\n",
      "157654/157654 [==============================] - 234s 1ms/step - loss: 0.4587 - acc: 0.7639 - val_loss: 0.4119 - val_acc: 0.7823\n",
      "Epoch 3/100\n",
      "157654/157654 [==============================] - 258s 2ms/step - loss: 0.4105 - acc: 0.7905 - val_loss: 0.3748 - val_acc: 0.8114\n",
      "Epoch 4/100\n",
      "157654/157654 [==============================] - 243s 2ms/step - loss: 0.3867 - acc: 0.8042 - val_loss: 0.3626 - val_acc: 0.8202\n",
      "Epoch 5/100\n",
      "157654/157654 [==============================] - 241s 2ms/step - loss: 0.3700 - acc: 0.8137 - val_loss: 0.3531 - val_acc: 0.8243\n",
      "Epoch 6/100\n",
      "157654/157654 [==============================] - 233s 1ms/step - loss: 0.3556 - acc: 0.8220 - val_loss: 0.3499 - val_acc: 0.8316\n",
      "Epoch 7/100\n",
      "157654/157654 [==============================] - 229s 1ms/step - loss: 0.3463 - acc: 0.8293 - val_loss: 0.3367 - val_acc: 0.8337\n",
      "Epoch 8/100\n",
      "157654/157654 [==============================] - 231s 1ms/step - loss: 0.3360 - acc: 0.8334 - val_loss: 0.3356 - val_acc: 0.8323\n",
      "Epoch 9/100\n",
      "157654/157654 [==============================] - 232s 1ms/step - loss: 0.3287 - acc: 0.8375 - val_loss: 0.3301 - val_acc: 0.8394\n",
      "Epoch 10/100\n",
      "157654/157654 [==============================] - 235s 1ms/step - loss: 0.3231 - acc: 0.8415 - val_loss: 0.3276 - val_acc: 0.8371\n",
      "Epoch 11/100\n",
      "157654/157654 [==============================] - 232s 1ms/step - loss: 0.3165 - acc: 0.8454 - val_loss: 0.3250 - val_acc: 0.8405\n",
      "Epoch 12/100\n",
      "157654/157654 [==============================] - 210s 1ms/step - loss: 0.3111 - acc: 0.8479 - val_loss: 0.3216 - val_acc: 0.8410\n",
      "Epoch 13/100\n",
      "157654/157654 [==============================] - 158s 1ms/step - loss: 0.3070 - acc: 0.8505 - val_loss: 0.3193 - val_acc: 0.8440\n",
      "Epoch 14/100\n",
      "157654/157654 [==============================] - 176s 1ms/step - loss: 0.3032 - acc: 0.8525 - val_loss: 0.3164 - val_acc: 0.8452\n",
      "Epoch 15/100\n",
      "157654/157654 [==============================] - 169s 1ms/step - loss: 0.2980 - acc: 0.8546 - val_loss: 0.3159 - val_acc: 0.8456\n",
      "Epoch 16/100\n",
      "157654/157654 [==============================] - 175s 1ms/step - loss: 0.2935 - acc: 0.8571 - val_loss: 0.3191 - val_acc: 0.8478\n",
      "Epoch 17/100\n",
      "157654/157654 [==============================] - 175s 1ms/step - loss: 0.2924 - acc: 0.8581 - val_loss: 0.3147 - val_acc: 0.8491\n",
      "Epoch 18/100\n",
      "157654/157654 [==============================] - 176s 1ms/step - loss: 0.2882 - acc: 0.8608 - val_loss: 0.3157 - val_acc: 0.8472\n",
      "Epoch 19/100\n",
      "157654/157654 [==============================] - 167s 1ms/step - loss: 0.2841 - acc: 0.8626 - val_loss: 0.3129 - val_acc: 0.8488\n",
      "Epoch 20/100\n",
      "157654/157654 [==============================] - 178s 1ms/step - loss: 0.2819 - acc: 0.8640 - val_loss: 0.3108 - val_acc: 0.8492\n",
      "Epoch 21/100\n",
      "157654/157654 [==============================] - 175s 1ms/step - loss: 0.2776 - acc: 0.8663 - val_loss: 0.3137 - val_acc: 0.8506\n",
      "Epoch 22/100\n",
      "157654/157654 [==============================] - 169s 1ms/step - loss: 0.2751 - acc: 0.8674 - val_loss: 0.3133 - val_acc: 0.8474\n",
      "Epoch 23/100\n",
      "157654/157654 [==============================] - 172s 1ms/step - loss: 0.2726 - acc: 0.8692 - val_loss: 0.3116 - val_acc: 0.8510\n",
      "Epoch 24/100\n",
      "157654/157654 [==============================] - 175s 1ms/step - loss: 0.2697 - acc: 0.8705 - val_loss: 0.3096 - val_acc: 0.8505\n",
      "Epoch 25/100\n",
      "157654/157654 [==============================] - 169s 1ms/step - loss: 0.2673 - acc: 0.8716 - val_loss: 0.3163 - val_acc: 0.8502\n",
      "Epoch 26/100\n",
      "157654/157654 [==============================] - 130s 824us/step - loss: 0.2671 - acc: 0.8726 - val_loss: 0.3119 - val_acc: 0.8529\n",
      "Epoch 27/100\n",
      "157654/157654 [==============================] - 83s 523us/step - loss: 0.2632 - acc: 0.8742 - val_loss: 0.3159 - val_acc: 0.8501\n",
      "Epoch 28/100\n",
      "157654/157654 [==============================] - 84s 535us/step - loss: 0.2613 - acc: 0.8749 - val_loss: 0.3115 - val_acc: 0.8502\n",
      "Epoch 29/100\n",
      "157654/157654 [==============================] - 82s 523us/step - loss: 0.2594 - acc: 0.8766 - val_loss: 0.3117 - val_acc: 0.8512\n",
      "Epoch 00029: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,epochs=100,callbacks=[early_stopping,checkpoint],\\\n",
    "              validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49268/49268 [==============================] - 8s 160us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31113756501564316, 0.84990257368349476]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:-1]:\n",
    "    embedding_model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_json = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_embedding_model.json','w') as f:\n",
    "    json.dump(embedding_model_json,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.save_weights('tweet_embedding_weights.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
